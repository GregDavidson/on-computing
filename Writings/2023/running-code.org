* How (Well) Does Your Code Run?

High-Level Programming Languages (HLLs) are designed for human convenience and
productivity, not for computer efficiency. Originally programs in HLLs were
prepared by human /Programmers/ and then given to human /Coders/ to translate
into /Assembly Language/, a symbolic form of the /Machine Language/ directly
understood by computer hardware. Now Coding is almost entirely automated, with
humans only manually writing small amounts of performance critical code in
Assembly Language.

** Compilers Translate HLL Code into LLL Code

A Compiler is a Software Application which can translate part or all of a
program written in a Higher Level Language (designed for humans) into a Lower
Level Language (designed for implementation efficiency).

There are generally different levels of optimization efficiency available for
the translation. A highly optimized translation of a time-critical section of
your code can make a big difference in your program's performance, but it will
also take more time to perform that translation, potentially slowing down your
development process as in the famous [[https://xkcd.com/303][xkcd: Compiling]] comic.

A common approach is to use a low level of optimization during program
development. Later, when changes to the program are rare and profile data is
available to identify the "hot spots", you can ask your compiler to create a
more optimized translation.

** Your Machine Language May Vary!

The Lowest level Language available to Programmers is the [[https://en.wikipedia.org/wiki/Machine_code][Machine Code]]
associated with the [[https://en.wikipedia.org/wiki/Instruction_set_architecture][Architecture]] of your processor. Code written in this Machine
Code can be read and processed by your hardware, i.e. your CPU or GPU chips.
Different processor families, e.g. 8086, ARM, PowerPC, RISC-V, etc. have very
different machine codes!

But most "Hardware Processors" do not execute your "machine code" directly!
Instead, they translate it to a still lower-level language which is then
executed by your processor chip's [[https://en.wikipedia.org/wiki/Microarchitecture][microarchitecture]]. Different models of
processing units which understand the same machine code can have /very different
efficiencies/ when processing the same code!

Different computers typically also differ in important resources such as cache,
inter-chip and external I/O interfaces, and more. Many of these resources are
now integrated into your processor "chips" which are now often multi-chip
modules in a single package. Many of these resources are implemented by "support
chips" on your motherboard. Some resources are external to your motherboard,
e.g. network file systems. The performance characteristics of all of these
resources and the efficiency with which they can be leveraged can drastically
effect the execution performance of your programs!

** You Might Want To Use An Intermediate Language

Some Intermediate Level Languages include
- Byte Code :: a sequence of 8-bit bytes which can be executed by an Interpreter Program
      - Byte Codes are often more compact than machine code
            - this can save on Memory and I/O resources
                  - Speeding up Program Load Time
                  - Reducing Paging and Swapping when memory resources are low
            - Software Interpretation of Byte Code is slower than Hardware Interpretation of Machine Code  generally run at less than 1/10th the speed of machine code
                  - By a factor of 10 (Tcl) to 50 (CPython)!
- Threaded Code :: a Sequence of Words holding the Addresses of Code to run
      - Each Threaded Code Instruction can point to
            - More Threaded Code or Machine Code
      - Threaded Code Instructions are
            - individually more powerful than /Byte Code/ or /Machine Code/ Instructions
            - intermediate in speed and size

Compilation (translation to a lower level code) can be done
- Ahead of time, before it meets your machine
      - Typical for commodity software
      - The code will have been compiled for a generic model of your processor architecture
- On your system
      - in advance of being run
            - maybe as a background process
            - utilizing spare cycles to do optimization
      - when you run it, as it's being loaded into main memory
      - while your program is running
            - often called "just in time compilation"
            - perhaps using profile metrics from the run so far

** WebAssembly is Taking Over Your Browser

Modern Web Browsers now implement an Intermediate Language called WebAssembly
- [[https://en.wikipedia.org/wiki/WebAssembly][Wikipedia's View]]
- [[https://developer.mozilla.org/en-US/docs/WebAssembly][Mozilla's View]]

WebAssembly is typically compiled to the machine language of your machine as or
after it's loaded into your browser's memory. All of the techniques (and
caveats) presented above (and more) apply. The translation can be guided by
realtime profiling, but it can't typically spend a lot of time doing
optimization or you'd notice your browser being slower rather than faster.

WebAssembly means that you can now write code for our websites in languages
other than JavaScript, e.g. Rust, various Lisps, etc., translate it into
WebAssembly offline and have your WebAssembly Code load and run as fast or
faster with your Web Pages than traditional JavaScript.

** Other Virtual Platforms

Many languages, including many Lisps compile to the Intermediate Languages used by popular Managed Virtual Machines, e.g.

- Java's JVM Platform
- MicroSoft's .Net Platform

As with WebAssembly, you compile your code ahead of time to the Intermediate
Level Language of your Virtual Platform. As and/or after your program is loaded
by that Platform, it will either be directly interpreted by software and/or
compiled to machine code and interpreted by your hardware.

These systems have become very sophisticated about profiling, detecting "hot
spots" and performing increased optimization resources where it will do the most
good. One of the consequences, is the "warmup" effect: When our software first
starts running on, e.g. a JVM Server Platform, it's not highly performant. After
that JVM has had a chance to profile and optimize your program's hot spots,
performance increases. This makes benchmarking difficult! And it's also why
there are different versions of the JVM: The version which runs one-shot
applications doesn't do as much of this expensive optimization in order to avoid
acting slow. The version that hosts long-running programs on servers does lots
of optimizations and therefore exhibits this warmup phenomenon.

*** References

** Profiling

- [[https://en.wikipedia.org/wiki/Profiling_(computer_programming)][Wikipedia: Profiling]]
- [[https://ftp.gnu.org/old-gnu/Manuals/gprof-2.9.1/html_mono/gprof.html][gprof: The GNU Profiler]]
-
** Compiling (Translating Code)

- [[https://en.wikipedia.org/wiki/Compiler][Wikipedia: Compiler]]
